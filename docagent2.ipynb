{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index import ServiceContext, StorageContext, load_index_from_storage, set_global_service_context\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "PERSIST_DIR = \"./storage2\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    llm = OpenAI(model_name=\"gpt-3.5-turbo\", token=os.environ[\"OPENAI_API_KEY\"])\n",
    "    service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=\"local\")\n",
    "    set_global_service_context(service_context)\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    docs = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(docs, service_context=service_context, storage_context=storage_context)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "\n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "docagent = \"DocAgent:\"\n",
    "exit_conditions = (\":q\", \"quit\", \"exit\", \"bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(query):\n",
    "    answer = query_engine.query(query)\n",
    "    if answer:\n",
    "        print(\"Doctor:\",query)\n",
    "        print(docagent,end=\" \")\n",
    "        print(answer.response.strip())\n",
    "        print()\n",
    "    else:\n",
    "        print(docagent, \"Sorry I can't help you with that. Try rephrasing your question.\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_searcher(query):\n",
    "    folder_path = 'Images'\n",
    "    image_embeddings = {}\n",
    "    image_paths = []\n",
    "\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        if (filename.endswith('.jpg') or filename.endswith('.jpeg')):\n",
    "            print(filename)\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            size_in_bytes = os.path.getsize(img_path)\n",
    "\n",
    "            # Convert bytes to kilobytes (1 KB = 1024 bytes)\n",
    "            size_in_kb = size_in_bytes / 1024\n",
    "            if size_in_kb<1000:\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                image_paths.append(img_path)\n",
    "\n",
    "    # Load images from your folder\n",
    "    images = [Image.open(img_path) for img_path in image_paths]\n",
    "\n",
    "    # Preprocess images and text using the model's processor\n",
    "    inputs = processor(text=query, images=images, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    image_features = outputs.image_embeds\n",
    "    text_features = outputs.text_embeds\n",
    "\n",
    "    # Calculate cosine similarities between text and image embeddings\n",
    "    similarities = torch.cosine_similarity(text_features, image_features)\n",
    "\n",
    "    # Get indices of top-k similar images\n",
    "    top_k = similarities.topk(k=2)  # Retrieve top 5 similar images\n",
    "    similar_image_indices = top_k.indices.tolist()\n",
    "\n",
    "    # Load and display retrieved images\n",
    "    similar_images = [images[index] for index in similar_image_indices]\n",
    "    for image in similar_images:\n",
    "        display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def identify_prompt_type(prompt):\n",
    "  \"\"\"\n",
    "  Identifies whether the given prompt is a text prompt or an image prompt.\n",
    "\n",
    "  Args:\n",
    "    prompt: The user-provided prompt.\n",
    "\n",
    "  Returns:\n",
    "    A string indicating the prompt type: \"text\" or \"image\".\n",
    "  \"\"\"\n",
    "  # Check for presence of keywords indicative of text retrieval\n",
    "  text_keywords = [\"what\", \"who\", \"why\", \"where\", \"when\", \"how\"]\n",
    "  text_match = any(keyword in prompt.lower() for keyword in text_keywords)\n",
    "\n",
    "  # Check for file extensions or image-related words\n",
    "  image_extensions = [\".jpg\", \".jpeg\", \".png\", \".gif\"]\n",
    "  image_words = [\"image\", \"picture\", \"photo\", \"visual\",\"report\"]\n",
    "  image_match = any(ext in prompt for ext in image_extensions) or \\\n",
    "               any(word in prompt.lower() for word in image_words)\n",
    "\n",
    "  # Determine prompt type based on matches\n",
    "  if image_match and not text_match:\n",
    "    return \"image\"\n",
    "  elif text_match and not image_match:\n",
    "    return \"text\"\n",
    "  else:\n",
    "    return \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please start entering your questions below. To quit, enter one of these keywords:  (':q', 'quit', 'exit', 'bye')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1536,) and (384,) not aligned: 1536 (dim 0) != 384 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m prompt_type \u001b[38;5;241m=\u001b[39m identify_prompt_type(query)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompt_type\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mtext_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m prompt_type\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     14\u001b[0m     image_searcher(query)\n",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m, in \u001b[0;36mtext_generator\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_generator\u001b[39m(query):\n\u001b[1;32m----> 2\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoctor:\u001b[39m\u001b[38;5;124m\"\u001b[39m,query)\n",
      "File \u001b[1;32mc:\\Users\\Raviteja Mandarapu\\OneDrive - Ashling Partners\\Desktop\\test\\DocAgentv2.0\\venvdoc\\lib\\site-packages\\llama_index\\core\\base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     39\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Raviteja Mandarapu\\OneDrive - Ashling Partners\\Desktop\\test\\DocAgentv2.0\\venvdoc\\lib\\site-packages\\llama_index\\query_engine\\retriever_query_engine.py:171\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    169\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    170\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 171\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[0;32m    173\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[0;32m    174\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    175\u001b[0m     )\n\u001b[0;32m    177\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n",
      "File \u001b[1;32mc:\\Users\\Raviteja Mandarapu\\OneDrive - Ashling Partners\\Desktop\\test\\DocAgentv2.0\\venvdoc\\lib\\site-packages\\llama_index\\query_engine\\retriever_query_engine.py:127\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[1;32m--> 127\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_node_postprocessors(nodes, query_bundle\u001b[38;5;241m=\u001b[39mquery_bundle)\n",
      "File \u001b[1;32mc:\\Users\\Raviteja Mandarapu\\OneDrive - Ashling Partners\\Desktop\\test\\DocAgentv2.0\\venvdoc\\lib\\site-packages\\llama_index\\core\\base_retriever.py:224\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    221\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[0;32m    222\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[0;32m    223\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[1;32m--> 224\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001b[0;32m    226\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[0;32m    227\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[0;32m    228\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Raviteja Mandarapu\\OneDrive - Ashling Partners\\Desktop\\test\\DocAgentv2.0\\venvdoc\\lib\\site-packages\\llama_index\\indices\\vector_store\\retrievers\\retriever.py:92\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle\u001b[38;5;241m.\u001b[39membedding_strs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     87\u001b[0m         query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39membed_model\u001b[38;5;241m.\u001b[39mget_agg_embedding_from_queries(\n\u001b[0;32m     89\u001b[0m                 query_bundle\u001b[38;5;241m.\u001b[39membedding_strs\n\u001b[0;32m     90\u001b[0m             )\n\u001b[0;32m     91\u001b[0m         )\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_nodes_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Raviteja Mandarapu\\OneDrive - Ashling Partners\\Desktop\\test\\DocAgentv2.0\\venvdoc\\lib\\site-packages\\llama_index\\indices\\vector_store\\retrievers\\retriever.py:168\u001b[0m, in \u001b[0;36mVectorIndexRetriever._get_nodes_with_embeddings\u001b[1;34m(self, query_bundle_with_embeddings)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_nodes_with_embeddings\u001b[39m(\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m, query_bundle_with_embeddings: QueryBundle\n\u001b[0;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m    167\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_vector_store_query(query_bundle_with_embeddings)\n\u001b[1;32m--> 168\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mquery(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_node_list_from_query_result(query_result)\n",
      "File \u001b[1;32mc:\\Users\\Raviteja Mandarapu\\OneDrive - Ashling Partners\\Desktop\\test\\DocAgentv2.0\\venvdoc\\lib\\site-packages\\llama_index\\vector_stores\\simple.py:274\u001b[0m, in \u001b[0;36mSimpleVectorStore.query\u001b[1;34m(self, query, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m     top_similarities, top_ids \u001b[38;5;241m=\u001b[39m get_top_k_mmr_embeddings(\n\u001b[0;32m    267\u001b[0m         query_embedding,\n\u001b[0;32m    268\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m         mmr_threshold\u001b[38;5;241m=\u001b[39mmmr_threshold,\n\u001b[0;32m    272\u001b[0m     )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m VectorStoreQueryMode\u001b[38;5;241m.\u001b[39mDEFAULT:\n\u001b[1;32m--> 274\u001b[0m     top_similarities, top_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_top_k_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid query mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Raviteja Mandarapu\\OneDrive - Ashling Partners\\Desktop\\test\\DocAgentv2.0\\venvdoc\\lib\\site-packages\\llama_index\\indices\\query\\embedding_utils.py:31\u001b[0m, in \u001b[0;36mget_top_k_embeddings\u001b[1;34m(query_embedding, embeddings, similarity_fn, similarity_top_k, embedding_ids, similarity_cutoff)\u001b[0m\n\u001b[0;32m     29\u001b[0m similarity_heap: List[Tuple[\u001b[38;5;28mfloat\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(embeddings_np):\n\u001b[1;32m---> 31\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m similarity_cutoff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m similarity_cutoff:\n\u001b[0;32m     33\u001b[0m         heapq\u001b[38;5;241m.\u001b[39mheappush(similarity_heap, (similarity, embedding_ids[i]))\n",
      "File \u001b[1;32mc:\\Users\\Raviteja Mandarapu\\OneDrive - Ashling Partners\\Desktop\\test\\DocAgentv2.0\\venvdoc\\lib\\site-packages\\llama_index\\core\\embeddings\\base.py:48\u001b[0m, in \u001b[0;36msimilarity\u001b[1;34m(embedding1, embedding2, mode)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(embedding1, embedding2)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     product \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(embedding1) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(embedding2)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m product \u001b[38;5;241m/\u001b[39m norm\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1536,) and (384,) not aligned: 1536 (dim 0) != 384 (dim 0)"
     ]
    }
   ],
   "source": [
    "print(\"Please start entering your questions below. To quit, enter one of these keywords: \", exit_conditions)\n",
    "while(True):\n",
    "    query = input(\"> \")\n",
    "    if(query in exit_conditions):\n",
    "        print(docagent, \"Thank you Doc, Happy to assist you!\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        prompt_type = identify_prompt_type(query)\n",
    "\n",
    "        if prompt_type==\"text\":\n",
    "            text_generator(query)\n",
    "        elif prompt_type==\"image\":\n",
    "            image_searcher(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
